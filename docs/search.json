[
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "data manipulation\nversion control platforms"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "data manipulation\nversion control platforms"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nessential dplyr functions and syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI still need help understanding the differences and uses of software we will be using"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe’re building foundations for understanding how to manipulate large tables of data"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\ncreating websites for storing information is a very interesting concept that I’ve never explored before"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nI am a first year graduate student here at UPenn pursuing my masters in urban spatial analytics.\n\n\n\n\nEmail: aruth3@upenn.edu\nGitHub: @aruth3"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "I am a first year graduate student here at UPenn pursuing my masters in urban spatial analytics."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: aruth3@upenn.edu\nGitHub: @aruth3"
  },
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Appendix.html",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Phase 1: Data Preparation\nIn this analysis, we attempt to measure what factors we can use to predict the sale prices of residential properties in Philadelphia. In order to prepare the dataset for modeling residential property sales, we applied a series of targeted cleaning and structural, socioeconomic, and spatial variable selection.\nWe filtered our property sales data to only show sales for the past two years to ensure up-to-date insights. We restricted the dataset to \"SINGLE FAMILY\" and \"MULTI FAMILY\" homes, excluding apartments and non-residential sales to ensure comparability, relevancy, and accuracy in our prediction models. Various apartments contained within the same building were found to be listed with a comprehensive sales price of the entire building which posed the threat of distorting our analysis. Observation values that were regarded as data entry errors or outliers were also removed. Properties with zero bathrooms, bedrooms, or livable area were removed as well as properties with a sales price below $10,000 and above $1,000,000.\nWe retained select structural attributes from our original dataset that we considered theoretically relevant to predicting sale price: number of bedrooms, number of bathrooms, total livable area, year the home was built, exterior condition, availability of garage spaces, and finally, the dependent variable, sale price.\nCode for Property Sales Data Cleaning and Variable Selection\n\n\nCode\n#load necessary libraries\nlibrary(modelsummary)\nlibrary(stargazer)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(knitr)\nlibrary(scales)\n\n#load census key for later use\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n#save data in url \nurl &lt;- \"https://phl.carto.com/api/v2/sql?filename=opa_properties_public&format=geojson&skipfields=cartodb_id&q=SELECT+*+FROM+opa_properties_public\"\n\n#suppress warnings for clarity and read data as spatial object\nsuppressWarnings({\n property_data &lt;- st_read(url)\n})\n\n#clean data\nparcel_data &lt;- property_data%&gt;%\n  select(location, #load columns that could potentially be used as predictors \n         category_code_description, #maybe garage_spaces and central air too?\n         number_of_bedrooms,\n         number_of_bathrooms, \n         total_livable_area,\n         year_built,\n         exterior_condition,\n         garage_spaces,\n         sale_price,\n         sale_date)%&gt;%\n  filter(category_code_description %in% \n  c(\"SINGLE FAMILY\",\"MULTI FAMILY\"))%&gt;% #no apartments, sales price of building\n  drop_na(number_of_bedrooms, #remove anomalies like houses with no rooms\n          number_of_bathrooms,\n          total_livable_area,\n          sale_price,\n          year_built) %&gt;%\n  filter(number_of_bedrooms&gt;0, \n         number_of_bathrooms&gt;0,\n         total_livable_area&gt;0, \n         sale_price&gt;=10000, sale_price &lt;=1000000)%&gt;% #remove very low/high prices\n  mutate(sale_year = str_remove(sale_date, \"-.*\"))%&gt;%   \n  filter(sale_year %in% c(\"2023\",\"2024\"))%&gt;% #limit to only 2023 and 2024\n  mutate(year_built = as.numeric(year_built))%&gt;%\n  mutate(Age = 2025 - year_built)%&gt;% filter(Age &lt;2000)%&gt;% #create a age column\n  filter(exterior_condition != 0) %&gt;% #create exterior condition binary  \n  mutate(\n    exterior_good = case_when(\n      exterior_condition &gt;= 1 & exterior_condition &lt;= 5 ~ 1,\n      exterior_condition &gt;= 6 & exterior_condition &lt;= 9 ~ 0,\n      TRUE ~ NA_real_ \n    )\n  )\n\n\n\n\n\nTable 1. Property Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nRaw Property Data\n583824\n79\n\n\nCleaned Parcel Data\n25268\n14\n\n\n\n\n\nWe derived our socioeconomic predictors from tract-level census data provided by the 2022 American Community Survey: median income, number with at least a bachelor’s degree, total number of those with at least a bachelor’s degree, number of those living in poverty, and total of those living in poverty. Census tracts with missing median income or zero reported as the median income were removed as it often indicated missing or zero values for other key predictors. We also mutated our census dataset to include two more columns, the percentage of people with bachelors and percentage of people in poverty, in order to standardize the observations across varying tract population sizes. Spatial data of the census tracts was also loaded in order to join our census variables to our parcel-level property data.\nCode for Census Data Cleaning and Variable Selection\n\n\nCode\n#load data about poverty(counts and total), bachelors(counts and total), and income \ncensus_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = c(\n    median_income = \"B19013_001\",\n    num_with_bach = \"B15003_022\",\n    bachelors_total =\"B15003_001\",\n    num_in_poverty = \"B17001_002\",\n    poverty_total =\"B17001_001\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n#create percentage columns for bachelors and poverty \nphilly_census &lt;- census_data%&gt;%\n  mutate(\n    percentage_bach = num_with_bachE / bachelors_totalE,\n    percentage_pov =  num_in_povertyE / poverty_totalE\n  )\n\n#remove data errors or incomplete fields \nphilly_census &lt;- philly_census%&gt;%\n  drop_na(median_incomeE)%&gt;%\n  filter(median_incomeE&gt;0)\n\n\n#spatial census data \nphiladelphia_tracts &lt;- tracts(\n  state = \"PA\",\n  county = \"Philadelphia\",\n  cb = TRUE,\n  year = 2022\n)\n\n#join census data and tract geometry to PARCEL data\nparcel_data &lt;- parcel_data %&gt;%\n  st_transform(st_crs(philadelphia_tracts))%&gt;%\n  st_join(philadelphia_tracts, join = st_within)%&gt;%\n  left_join(philly_census, by = \"GEOID\")\n\n\n\n\n\nTable 2. Census Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nCensus Data\n408\n12\n\n\nCleaned Census Data\n383\n14\n\n\n\n\n\nTo further contextualize house prices, we loaded spatial datasets including the locations of colleges and universities, 2024 crime incidents, and Philadelphia neighborhood boundaries. These datasets were loaded with the intention of engineering spatial features such as proximity measurements and neighborhood stratification in order to account for spatial patterns and potential interactive spatial effects within our predictive model. Only the 2024 crime incidents needed to be cleaned due to the missing geometries that would prevent spatial analysis.\nCode for Spatial Data Cleaning and Variable Selection\n\n\nCode\n#load university data\nuniversity_data &lt;- st_read(\"./data/Universities_Colleges.geojson\")\n\n#load 2024 crime incident data\norg_crime_data &lt;-st_read(\"./data/incidents_part1_part2.shp\")\n\n#removed crime incidents with no geometry \ncrime_data &lt;- org_crime_data %&gt;%\n  filter(!st_is_empty(geometry))\n\n#load neighborhood data \nneighborhoods &lt;- st_read(\"./data/philadelphia-neighborhoods.shp\")\n\n\n\n\n\nTable 3. Spatial Datasets Dimensions Before and After Cleaning\n\n\nDataset\nRows\nColumns\n\n\n\n\nCrime Data\n160388\n14\n\n\nCleaned Crime Data\n153644\n14\n\n\n\n\n\n\n\nPhase 2: Exploratory Data Analysis\nWe created a histogram to visualize the distribution of home sale prices in Philadelphia. The resulting graph revealed a positively skewed distribution, indicating that most properties were sold at lower price points, primarily between $150,000 and $350,000, while a small number of high-value homes sold for up to $1,000,000. As it pertains to our predictive model, this skewness suggests that a log-transformation of sale prices may be necessary to normalize the distribution of values and potentially improve model performance. It also indicates the need for diagnostic checks to determine the impact of outliers in order to ensure that the few high-priced homes do not distort our model estimates.\n\n\n\n\n\n\n\n\n\nWe also explored the geographic distribution of sales prices across the neighborhoods in Philadelphia. We calculated the median sales price by each Philadelphia neighborhood. The map in Figure 2 shows distinct spatial patterns with higher prices above $400,000, represented by lighter colors, being concentrated in neighborhoods such as Center City, University City, and parts of Northwest Philadelphia.\n\n\n\n\n\n\n\n\n\nOne interesting relationship we found when exploring the structural predictors of sales price was the non-linear association between home age and sale price. Because we have already determined that sale prices are not normally distributed, we plotted our predictors by the log-transformed sale price to limit distortion when attempting to assess the relationship between predictors and sales price. Even with this transformation, Figure 3 shows that sale prices tend to decline for middle-aged homes but rebound for older, more historic properties. The figure includes both a linear and quadratic line to further emphasize that a linear relationship is not sufficient in capturing the impact of age on house prices.\n\n\n\n\n\n\n\n\n\nOne spatial factor that we found theoretically relevant to predicting sales price was proximity to violent crimes. We hypothesized that a home’s proximity to violent crimes such as homicides, aggravated assaults with and without firearms, and armed robberies may lead to a depreciation in house value. To measure this, loaded the location of crime incidents 2024 and calculated the density of crime within 600 feet of each home. In Figure 4 we explored this relationship between violent crimes and found a general negative trend between proximity to crime and sale price.\n\n\n\n\n\n\n\n\n\nWe also created a violin plot to illustrates the distribution and density of sale prices across within the top ten neighborhoods with the highest median sale prices. The violin plot in Figure 5 reveals heterogeneity in sale prices within the wealthiest neighborhoods, indicating juxtaposition of high and low sales prices within close proximity of one another. While some neighborhoods are more concentrated at similar sales prices, neighborhoods like Bella Vista and Northern Liberties had long tails extending towards the lowest sale prices.\n\n\n\n\n\n\n\n\n\n\n\nPhase 3: Feature Engineering\nIn order to account for the spatial patterns highlight during the exploratory phase, we created a number of spatial features to incorporate into our predictive model.\nCode for Feature Engineering: Crime Buffer\n\n\nCode\n#set crs for distance calculations\ncrime_proj &lt;- st_transform(crime_data, 3365)\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nuniversity_proj &lt;- st_transform(university_data, 3365)\n\n#proximity (within 600 feet) to violent crime\n\nparcel_buffers&lt;- st_buffer(parcel_proj, dist=600)\n\nviolent_crimes &lt;- c(\"Homicide - Criminal\",\"Aggravated Assault No Firearm\",\"Robbery Firearm\",\"Aggravated Assault Firearm\")\n\nviolent_proj &lt;- crime_proj %&gt;%\n  filter(text_gener %in% violent_crimes)\n    \nviolent_crime_counts &lt;- st_intersects(parcel_buffers, violent_proj)\nviolent_crime_counts &lt;- lengths(violent_crime_counts)\nparcel_data$violent_crime_600ft &lt;- violent_crime_counts\n\n\nCode for Feature Engineering: K-Nearest Neighbor for Colleges\n\n\nCode\n#knn to university \n\nuniversity_proj &lt;- st_transform(university_data, 3365) #projection changed in order to calculate distance \n\n\ndist_matrix &lt;- st_distance(parcel_proj, university_proj)\n\n\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\nparcel_data$college_nn1 &lt;- get_knn_distance(dist_matrix, k = 1)\nparcel_data$college_nn3 &lt;- get_knn_distance(dist_matrix, k = 3)\nparcel_data$college_nn5 &lt;- get_knn_distance(dist_matrix, k = 5)\n\n#determine which nearest neighbor is correlated the most with sales price \n\nparcel_data %&gt;% \n  st_drop_geometry() %&gt;%\n  select(sale_price, college_nn1, college_nn3, college_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(sale_price)\n\n\nCode for Feature Engineering: Neighborhood Interaction Effects\n\n\nCode\n# wealthy neighborhood interaction effects\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nneighborhood_proj &lt;-st_transform(neighborhoods, 3365)\n\n\n#median sales price \nwealthy_neighborhoods &lt;- parcel_with_neighborhood%&gt;%\n  filter(median_price &gt;= 275500)%&gt;%\n  pull(NAME)\n\n#finding the parcels that fall within neighborhood boundaries \nparcel_data &lt;- parcel_proj %&gt;%\n  st_join(neighborhood_proj, join = st_within) %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(NAME %in% wealthy_neighborhoods, \"Wealthy\", \"Not Wealthy\"), #creating a flag to delineate them into two categories \n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n\n\n\n\nTable 4. Summary of Engineered Features\n\n\n\n\n\n\n\n\nFeature_Name\nFeature_Type\nDescription\nJustification\n\n\n\n\nviolent_crime_600ft\nBuffer-based (spatial)\nCount of violent crimes within 600 ft of parcel\nCaptures neighborhood safety and crime exposure\n\n\ncollege_nn1\nkNN (spatial distance)\nAverage distance to nearest college/university (k=1)\nMeasures proximity to educational amenities\n\n\ncollege_nn3\nkNN (spatial distance)\nAverage distance to 3 nearest colleges/universities\nCaptures local educational accessibility\n\n\ncollege_nn5\nkNN (spatial distance)\nAverage distance to 5 nearest colleges/universities\nCaptures broader proximity to higher education hubs\n\n\nmedian_incomeE\nCensus (socioeconomic)\nMedian household income of census tract\nRepresents economic conditions of local area\n\n\npercentage_bach\nCensus (education)\nPercent of tract population with bachelor’s degree\nEducation level could be connected to property values\n\n\npercentage_pov\nCensus (poverty)\nPercent of tract population below poverty line\nReflects socioeconomic disadvantage\n\n\nwealthy_neighborhood\nInteraction term (categorical)\nIndicates parcels located in wealthy neighborhoods\nIdentifies context of neighborhood wealth levels\n\n\nlog_sale_price\nTransformation (continuous)\nNatural log of sale price\nStabilizes variance for regression modeling\n\n\nlog_livable_area\nTransformation (continuous)\nNatural log of total livable area (sq ft)\nNormalizes skewed size variable for modeling\n\n\nAge\nStructural (numeric)\nYears since building construction\nAccounts for devaluation of property value with time\n\n\nexterior_good\nCondition (binary)\n1 = good/average exterior, 0 = poor\nChanged categorical condition to numeric to understand exterior desirability\n\n\n\n\n\nThe features included were engineered to capture social, economic, and environmental factors influencing housing prices. Buffer-based and nearest-neighbor measures quantify local accessibility and safety. Census data enrich parcels with socio-economic context. Interaction terms account for neighborhood-level wealth effects which helps contextualize external features of a home, for instance homes in a wealthy neighborhood are more likely to have a higher property value than those not in a wealthy neighborhood. Applying a binary indicator improves model interpretability and performance.\n\n\nPhase 4: Model Building\nWhen building our model’s be progressively built out models to expand upon the last with out first model being the baseline of all our models and using only structural variables, the second model adding socioeconomic census variables, our third model introducing our spatial features, and our fourth model added spatial interactions and fixed effects.\nCode for Model Building\n\n\nCode\n#create log livable area for modeling\n\nparcel_data &lt;- parcel_data%&gt;%\n  mutate(log_livable_area = log(total_livable_area))\n\n#structural features only \nmodel1 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area + garage_spaces + Age + I(Age^2) + exterior_good, data = parcel_data)\n\n\n#structural + census \nmodel2 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area  + garage_spaces + Age + I(Age^2) + exterior_good + median_incomeE + percentage_bach + percentage_pov, data = parcel_data)\n\n#structural + census + spatial \nmodel3 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\n\n#structural + census + spatial + interactions \nmodel4 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area * wealthy_neighborhood + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\n\nOur summary table of coefficients for predictors by model indicate statistical numerically as well as with stars to account for low values omitted through the rounding of coefficients.\nFull Model Summary of Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n5.512***\n7.275***\n7.615***\n7.175***\n\n\n\n[5.254, 5.771]\n[7.060, 7.491]\n[7.429, 7.802]\n[6.933, 7.418]\n\n\nnumber_of_bathrooms\n0.283***\n0.215***\n0.213***\n0.209***\n\n\n\n[0.269, 0.296]\n[0.204, 0.226]\n[0.202, 0.224]\n[0.199, 0.220]\n\n\nnumber_of_bedrooms\n-0.192***\n-0.013*\n\n\n\n\n\n[-0.204, -0.180]\n[-0.023, -0.003]\n\n\n\n\nlog_livable_area\n0.874***\n0.497***\n0.461***\n0.515***\n\n\n\n[0.838, 0.910]\n[0.467, 0.528]\n[0.437, 0.484]\n[0.483, 0.548]\n\n\ngarage_spaces\n0.104***\n0.098***\n0.050***\n0.051***\n\n\n\n[0.089, 0.119]\n[0.085, 0.110]\n[0.037, 0.062]\n[0.039, 0.064]\n\n\nAge\n-0.012***\n-0.004***\n-0.004***\n-0.003***\n\n\n\n[-0.013, -0.011]\n[-0.004, -0.003]\n[-0.005, -0.003]\n[-0.004, -0.003]\n\n\nI(Age^2)\n0.000***\n0.000***\n0.000***\n0.000***\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n\n\nexterior_good\n1.180***\n0.941***\n0.881***\n0.877***\n\n\n\n[1.094, 1.265]\n[0.871, 1.011]\n[0.813, 0.950]\n[0.809, 0.944]\n\n\nmedian_incomeE\n\n0.000***\n0.000***\n0.000***\n\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n\n\npercentage_bach\n\n1.564***\n1.464***\n1.051***\n\n\n\n\n[1.474, 1.655]\n[1.373, 1.555]\n[0.956, 1.147]\n\n\npercentage_pov\n\n-0.862***\n-0.476***\n-0.375***\n\n\n\n\n[-0.933, -0.790]\n[-0.549, -0.403]\n[-0.447, -0.302]\n\n\ncollege_nn1\n\n\n0.000***\n0.000***\n\n\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n\n\nviolent_crime_600ft\n\n\n-0.020***\n-0.018***\n\n\n\n\n\n[-0.021, -0.019]\n[-0.019, -0.017]\n\n\nwealthy_neighborhoodWealthy\n\n\n\n1.093***\n\n\n\n\n\n\n[0.815, 1.372]\n\n\nlog_livable_area × wealthy_neighborhoodWealthy\n\n\n\n-0.118***\n\n\n\n\n\n\n[-0.157, -0.079]\n\n\nNum.Obs.\n24556\n24453\n24453\n24453\n\n\nR2\n0.351\n0.569\n0.589\n0.600\n\n\nR2 Adj.\n0.350\n0.569\n0.589\n0.600\n\n\nF\n1892.735\n3226.742\n3189.486\n2821.134\n\n\nRMSE\n0.61\n0.50\n0.48\n0.48\n\n\n\n\np &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nAcross all models, the coefficient for the structural predictors like the number of bathrooms, garage spaces, livable area, and exterior conditions remains consistently positive and statistically significant, with livable area and exterior conditions being the most influential of the predictors. The coefficients for age and it’s quadratic adjustment showed that older homes sold for less while the coefficients for the number of bedrooms showed that it became less statistically significant as non-structural predictors were introduced.\nAs census variables are introduced in Model 2, median income, educational attainment, and poverty rate all show expected relationships: higher income and education levels are associated with higher sale prices, while higher poverty rates are negatively associated.Our spatial features introduced in Model 3, distance from the nearest college and density of crime were also identified as statistically significant. In Model 4, the coefficient for wealthy neighborhood is large and positive. There is, however, a slight negative interaction between log(livable area) and wealthy neighborhoods. This suggests that while larger homes still tend to sell for more, the appreciation of value of each additional square foot is less pronounced in already expensive areas.\nOverall, the progression from Model 1 to Model 4 shows increasing explanatory power (R² rising from 0.35 to 0.60), and decreasing RMSE, indicating improved model fit as more structural and contextual variables are added.\n\n\nPhase 5: Model Validation\nIn order to further assess the predictive power of our models, we performed 10-fold cross validation for each model in order to determine how well the model predicts for unseen data.\nCode for 10-fold Cross-Validation\n\n\nCode\nparcel_data &lt;- na.omit(parcel_data)\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 10  # intiate 10-fold CV\n)\n\nmodel_cv1 &lt;-  train(log_sale_price ~ \n                      number_of_bathrooms +\n                      number_of_bedrooms +\n                      log_livable_area +\n                      garage_spaces + \n                      Age + I(Age^2) + \n                      exterior_good,\n                     data = parcel_data,\n                       method = \"lm\",\n                       trControl = ctrl)\n\n  \nmodel_cv2 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     number_of_bedrooms + \n                     log_livable_area  +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     median_incomeE + \n                     percentage_bach + \n                     percentage_pov,\n                     data = parcel_data,\n                      method = \"lm\",\n                      trControl = ctrl\n)\n\n\nmodel_cv3 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     log_livable_area +\n                     median_incomeE + \n                     percentage_bach +\n                     percentage_pov + \n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                    method = \"lm\",\n                    trControl = ctrl\n)\n\n\n\nmodel_cv4 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms+\n                     garage_spaces + \n                     Age + I(Age^2) +\n                     exterior_good + \n                     log_livable_area * wealthy_neighborhood + \n                     median_incomeE +\n                     percentage_bach + \n                     percentage_pov +\n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                      method = \"lm\",\n                    trControl = ctrl\n)\n\n\nBased on each model’s measurement average squared prediction error (RMSE), average absolute difference between predicted and actual values (MAE), and reported proportion of explained variance (R^2), we determined that Model 4’s inclusion of structural, socioeconomic, spatial, and interactive effects all contributed to smaller errors between predicted values and actual values and offered greater explanatory power.\n\n\n\nTable 6. Cross-Validated Performance Metrics for Four Models\n\n\nModel\nRMSE\nRsquared\nMAE\n\n\n\n\nModel 1\n0.6077\n0.3508\n0.4540\n\n\nModel 2\n0.4952\n0.5690\n0.3540\n\n\nModel 3\n0.4834\n0.5894\n0.3417\n\n\nModel 4\n0.4769\n0.6001\n0.3354\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph of the actual logged sale prices by the predicted logged sale prices compliments our model performance metrics found in Table 6 as points cluster closely around the reference line suggesting stable consistency in errors and moderate ability to predict sale prices.\n\n\nPhase 6: Model Diagnostics\nWhen using linear regression to model relationships, we assume a number of conditions are met to support the validity of using a linear model. Here we test various linear modeling assumptions to determine the appropriateness of our model.\nOne linear model assumption is the assumption that the relationship is actually linear. If the linearity assumption is met, residuals should be randomly scattered around the zero line with no discernible pattern.\nIn Figure 7, we plotted the residuals or prediction errors by the model’s fitted values. The red dashed line at zero marks where residuals would fall if predictions were perfect. In this plot, the residuals appear to fan out as fitted values increase, suggesting a possible violation of the linearity. This pattern may indicate that the model under- or over-predicts at different price levels.\n\n\n\n\n\n\n\n\n\nAnother assumption we test is the normal distribution of residuals or errors. In Figure 8, we test the assumption of normally distributed residuals using a Q-Q plot. If normality of residual’s is met, the sample quantile points should fall roughly along the blue reference line representing theoretical quantiles of normal distribution.\nIn this plot, the residuals deviate from the line, particularly in the tails at low and high values, indicating non-normality.\n\n\n\n\n\n\n\n\n\nOne last regression assumption check we performed was determining whether there were influential outliers present. Figure 9 displays Cook’s Distance values for each observation, with colors indicating whether the point is flagged as influential, blue being none influential observations and red being the influential obeservations. A substantial large number of our observations were marked as influential, suggesting that many data points have both high leverage and large residuals.\n\n\n\n\n\n\n\n\n\n\n\nPhase 7: Conclusions & Recommendations\nThe final model demonstrates strong performance, achieving an adjusted R² of 0.591 and explaining nearly 59% of the variation in housing prices across Philadelphia. This marks a substantial improvement over the baseline structural model (Model 1, R² = 0.35), highlighting the value of incorporating spatial characteristics alongside structural and socioeconomic factors.\nAmong all predictors, total livable area emerges as the most influential driver of price, followed by the number of bathrooms and garage spaces. Exterior condition, median income, and educational attainment also show positive associations. Conversely, poverty rate and local crime density are negatively associated with price.\nThe interaction between living area and wealthy neighborhoods reveal that additional square footage may not have as much of an impact on price in high-value areas. Spatial dependencies are also evident in the model’s predictive accuracy as prediction errors show spatial patterns. The model performs best in mid-range price neighborhoods but struggles in areas like Nicetown, Fairhill, and Upper Kensington, where residual errors are higher. These disparities raise important equity concerns in the context of policy as these areas that are predisposed to under-prediction in modeling are also historical disadvantaged neighborhoods."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html",
    "href": "assignments/assignment_1/assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#scenario",
    "href": "assignments/assignment_1/assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography =\"county\",\n  variables= c(\n     median_income =\"B19013_001\",\n     total_pop=\"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\ncounty_data &lt;- county_data%&gt;%\n  mutate(county_name = str_remove(NAME, \" County, Pennsylvania\"))\n         \n# Display the first few rows\nglimpse(county_data)\n\nRows: 67\nColumns: 7\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ county_name    &lt;chr&gt; \"Adams\", \"Allegheny\", \"Armstrong\", \"Beaver\", \"Bedford\",…"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_data &lt;-county_data%&gt;%\n  mutate(MOE_PCT=(median_incomeM/median_incomeE)*100,\n  reliability = case_when(\n    MOE_PCT &lt; 5 ~ \"High Confidence (&lt;5%)\",\n    MOE_PCT &lt;= 10 ~ \"Moderate Confidence (5%-10%)\",\n    TRUE ~ \"Low Confidence (&gt;10%)\"\n    )\n  )\ncounty_data\n\n# A tibble: 67 × 9\n   GEOID NAME    median_incomeE median_incomeM total_popE total_popM county_name\n   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n 1 42001 Adams …          78975           3334     104604         NA Adams      \n 2 42003 Allegh…          72537            869    1245310         NA Allegheny  \n 3 42005 Armstr…          61011           2202      65538         NA Armstrong  \n 4 42007 Beaver…          67194           1531     167629         NA Beaver     \n 5 42009 Bedfor…          58337           2606      47613         NA Bedford    \n 6 42011 Berks …          74617           1191     428483         NA Berks      \n 7 42013 Blair …          59386           2058     122640         NA Blair      \n 8 42015 Bradfo…          60650           2167      60159         NA Bradford   \n 9 42017 Bucks …         107826           1516     645163         NA Bucks      \n10 42019 Butler…          82932           2164     194562         NA Butler     \n# ℹ 57 more rows\n# ℹ 2 more variables: MOE_PCT &lt;dbl&gt;, reliability &lt;chr&gt;\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nreliability_summary &lt;- county_data%&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n/sum(n) * 100, 1),\n    total = sum(n)\n  )\nreliability_summary\n\n# A tibble: 2 × 4\n  reliability                      n percentage total\n  &lt;chr&gt;                        &lt;int&gt;      &lt;dbl&gt; &lt;int&gt;\n1 High Confidence (&lt;5%)           57       85.1    67\n2 Moderate Confidence (5%-10%)    10       14.9    67"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop_5_counties &lt;- county_data %&gt;%\n  arrange(desc(MOE_PCT))%&gt;%\n  slice_head(n=5)\n\n# Format as table with kable() - include appropriate column names and caption \ntop_5_counties %&gt;%\n  select(county_name, median_incomeE, median_incomeM, MOE_PCT, reliability)%&gt;%\n    kable(\n      col.names = c(\"County Name\", \"Income ($)\", \"MOE\", \"MOE Percentage (%)\", \"MOE Reliability\"),\n      caption = \"Top 5 Counties Exhibiting High Uncertainty\",\n     digits=c(0,0,1,1,0)\n)\n\n\nTop 5 Counties Exhibiting High Uncertainty\n\n\n\n\n\n\n\n\n\nCounty Name\nIncome ($)\nMOE\nMOE Percentage (%)\nMOE Reliability\n\n\n\n\nForest\n46188\n4612\n10.0\nModerate Confidence (5%-10%)\n\n\nSullivan\n62910\n5821\n9.3\nModerate Confidence (5%-10%)\n\n\nUnion\n64914\n4753\n7.3\nModerate Confidence (5%-10%)\n\n\nMontour\n72626\n5146\n7.1\nModerate Confidence (5%-10%)\n\n\nElk\n61672\n4091\n6.6\nModerate Confidence (5%-10%)\n\n\n\n\n\nData Quality Commentary:\nThese results highlight that although all five counties are all considered to indicate moderate confidence in their income estimations, their margins of error vary significantly. For instance, the county with the largest margin of error, Forest County, lies on the cusp of exhibiting low confidence while the smallest margin of error, Elk County, is much closer to being high confidence than low. These discrepancies, in addition to general uncertainty, indicate a need for additional oversight when using income data from these counties for decision-making."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties &lt;- county_data %&gt;% \n  filter(county_name %in% c(\"Allegheny\", \"Carbon\", \"Forest\"))   %&gt;%\n  select(county_name, median_incomeE, MOE_PCT, reliability)\n\nselected_counties\n\n# A tibble: 3 × 4\n  county_name median_incomeE MOE_PCT reliability                 \n  &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                       \n1 Allegheny            72537    1.20 High Confidence (&lt;5%)       \n2 Carbon               64538    5.31 Moderate Confidence (5%-10%)\n3 Forest               46188    9.99 Moderate Confidence (5%-10%)\n\n\nComment on the output: Because my data possessed no findings with low confidence, I chose the top county from my “Top 5 Counties Exhibiting High Uncertainty” as my low confidence observation. A gradient in reliability, however, is still achieved as, as previously observed, the margins of error between both counties with estimations of moderate confidence range drastically."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nfind_codes &lt;- county_data %&gt;% \n  select(county_name, GEOID)%&gt;%\n  filter(county_name %in% c(\"Allegheny\", \"Carbon\", \"Forest\"))   %&gt;%\n  mutate(county_code = str_remove(GEOID, \"42\"))\nfind_codes\n\n# A tibble: 3 × 3\n  county_name GEOID county_code\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;      \n1 Allegheny   42003 003        \n2 Carbon      42025 025        \n3 Forest      42053 053        \n\ntract_data &lt;- get_acs(\n  geography =\"tract\",\n  variables= c(\n     white =\"B03002_003\",\n     black=\"B03002_004\",\n     hispanic=\"B03002_012\",\n     total_pop=\"B03002_001\"\n  ),\n  state = my_state,\n  county =c(003, 025, 053),\n  year = 2022,\n  output = \"wide\"\n)\n\ntract_data\n\n# A tibble: 413 × 10\n   GEOID       NAME   whiteE whiteM blackE blackM hispanicE hispanicM total_popE\n   &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 42003010301 Censu…    755    108   1086    125        60        47       2028\n 2 42003010302 Censu…   3283    254    678    174       269       147       4631\n 3 42003020100 Censu…   2915    319    541    142       192       102       4310\n 4 42003020300 Censu…   1170    201     15     23        44        32       1471\n 5 42003030500 Censu…    724    179   1821    451       135       101       3044\n 6 42003040200 Censu…    947    234    493    140        53        30       1843\n 7 42003040400 Censu…   1192    216     61     49        17        21       1629\n 8 42003040500 Censu…   2342    404     79     63        88        56       2840\n 9 42003040600 Censu…   1928    417     69     63        67        60       2302\n10 42003040900 Censu…   2496    409    609    248        92        63       3722\n# ℹ 403 more rows\n# ℹ 1 more variable: total_popM &lt;dbl&gt;\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n# Add readable tract and county name columns using str_extract() or similar\n\nunreadable= str_split(tract_data$NAME, \";\", simplify =  TRUE)\n\ntract_data &lt;-tract_data%&gt;% \n  mutate(white_pct=(whiteE/total_popE)*100,\n         black_pct=(blackE/total_popE)*100,\n         hispanic_pct=(hispanicE/total_popE)*100,\n         tract_name=unreadable[,1],\n         county_name=str_remove(unreadable[,2], \"County\")\n        \n)\n\nglimpse(tract_data)\n\nRows: 413\nColumns: 15\n$ GEOID        &lt;chr&gt; \"42003010301\", \"42003010302\", \"42003020100\", \"42003020300…\n$ NAME         &lt;chr&gt; \"Census Tract 103.01; Allegheny County; Pennsylvania\", \"C…\n$ whiteE       &lt;dbl&gt; 755, 3283, 2915, 1170, 724, 947, 1192, 2342, 1928, 2496, …\n$ whiteM       &lt;dbl&gt; 108, 254, 319, 201, 179, 234, 216, 404, 417, 409, 57, 150…\n$ blackE       &lt;dbl&gt; 1086, 678, 541, 15, 1821, 493, 61, 79, 69, 609, 1450, 132…\n$ blackM       &lt;dbl&gt; 125, 174, 142, 23, 451, 140, 49, 63, 63, 248, 404, 422, 3…\n$ hispanicE    &lt;dbl&gt; 60, 269, 192, 44, 135, 53, 17, 88, 67, 92, 3, 0, 84, 98, …\n$ hispanicM    &lt;dbl&gt; 47, 147, 102, 32, 101, 30, 21, 56, 60, 63, 8, 11, 73, 54,…\n$ total_popE   &lt;dbl&gt; 2028, 4631, 4310, 1471, 3044, 1843, 1629, 2840, 2302, 372…\n$ total_popM   &lt;dbl&gt; 61, 198, 368, 206, 535, 235, 220, 423, 446, 452, 401, 377…\n$ white_pct    &lt;dbl&gt; 37.2287968, 70.8918160, 67.6334107, 79.5377294, 23.784494…\n$ black_pct    &lt;dbl&gt; 53.5502959, 14.6404664, 12.5522042, 1.0197145, 59.8226018…\n$ hispanic_pct &lt;dbl&gt; 2.9585799, 5.8086806, 4.4547564, 2.9911625, 4.4349540, 2.…\n$ tract_name   &lt;chr&gt; \"Census Tract 103.01\", \"Census Tract 103.02\", \"Census Tra…\n$ county_name  &lt;chr&gt; \" Allegheny \", \" Allegheny \", \" Allegheny \", \" Allegheny …"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhighest_hispanic_pct &lt;- tract_data %&gt;%\n  arrange(desc(hispanic_pct))%&gt;%\n  slice(1)\nglimpse(highest_hispanic_pct)\n\nRows: 1\nColumns: 15\n$ GEOID        &lt;chr&gt; \"42003120300\"\n$ NAME         &lt;chr&gt; \"Census Tract 1203; Allegheny County; Pennsylvania\"\n$ whiteE       &lt;dbl&gt; 33\n$ whiteM       &lt;dbl&gt; 38\n$ blackE       &lt;dbl&gt; 1678\n$ blackM       &lt;dbl&gt; 494\n$ hispanicE    &lt;dbl&gt; 343\n$ hispanicM    &lt;dbl&gt; 499\n$ total_popE   &lt;dbl&gt; 2093\n$ total_popM   &lt;dbl&gt; 681\n$ white_pct    &lt;dbl&gt; 1.576684\n$ black_pct    &lt;dbl&gt; 80.172\n$ hispanic_pct &lt;dbl&gt; 16.38796\n$ tract_name   &lt;chr&gt; \"Census Tract 1203\"\n$ county_name  &lt;chr&gt; \" Allegheny \"\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\nsummary_by_county &lt;- tract_data %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    number_of_tracts=n(),\n    mean_white_pct=round(mean(white_pct, na.rm = TRUE),1),\n    mean_black_pct=round(mean(black_pct, na.rm = TRUE),1),\n    mean_hispanic_pct=round(mean(hispanic_pct, na.rm = TRUE),1)\n  )\nsummary_by_county\n\n# A tibble: 3 × 5\n  county_name   number_of_tracts mean_white_pct mean_black_pct mean_hispanic_pct\n  &lt;chr&gt;                    &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 \" Allegheny \"              394           74.5           15.4               2.4\n2 \" Carbon \"                  17           89              1.9               6.4\n3 \" Forest \"                   2           71.2           13.6               7.4\n\n# Create a nicely formatted table of your results using kable()\nsummary_by_county%&gt;%\n kable(\n      col.names = c(\"County Name\", \"Total Tracts Per County\", \"Average of White Residents(%)\", \"Average of Black Residents(%)\", \"Average of Hispanic Residents (%)\"),\n      caption = \"Average Demographics Per County\",\n     digits=c(0,0,1,1,1)\n)\n\n\nAverage Demographics Per County\n\n\n\n\n\n\n\n\n\nCounty Name\nTotal Tracts Per County\nAverage of White Residents(%)\nAverage of Black Residents(%)\nAverage of Hispanic Residents (%)\n\n\n\n\nAllegheny\n394\n74.5\n15.4\n2.4\n\n\nCarbon\n17\n89.0\n1.9\n6.4\n\n\nForest\n2\n71.2\n13.6\n7.4"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ntract_data &lt;-tract_data%&gt;%\n  mutate(\n    white_moe_pct=(whiteM/whiteE)*100,\n    black_moe_pct=(blackM/blackE)*100,\n    hispanic_moe_pct=(hispanicM/hispanicE)*100,\n    moe_flag=ifelse(\n    white_moe_pct &gt; 15 |\n    black_moe_pct &gt; 15 |\n    hispanic_moe_pct &gt; 15,\n    TRUE, FALSE)\n)\n\n# Create summary statistics showing how many tracts have data quality issues\n quality_summary &lt;- tract_data%&gt;%\n  count(moe_flag) %&gt;%\n  mutate(\n    percentage = round(n/sum(n) * 100, 1),\n    total = sum(n)\n  )\nquality_summary\n\n# A tibble: 1 × 4\n  moe_flag     n percentage total\n  &lt;lgl&gt;    &lt;int&gt;      &lt;dbl&gt; &lt;int&gt;\n1 TRUE       413        100   413"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\nsummary_by_moe &lt;- tract_data %&gt;%\n  group_by(moe_flag) %&gt;%\n  summarize(\n    number_of_tracts= n(),\n    mean_population=round(mean(total_popE, na.rm = TRUE),0),\n    mean_white_pct=round(mean(white_pct, na.rm = TRUE),1),\n    mean_black_pct=round(mean(black_pct, na.rm = TRUE),1),\n    mean_hispanic_pct=round(mean(hispanic_pct, na.rm = TRUE),1)\n  )\nsummary_by_moe\n\n# A tibble: 1 × 6\n  moe_flag number_of_tracts mean_population mean_white_pct mean_black_pct\n  &lt;lgl&gt;               &lt;int&gt;           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 TRUE                  413            3190             75           14.8\n# ℹ 1 more variable: mean_hispanic_pct &lt;dbl&gt;\n\n# Create a professional table showing the patterns\nsummary_by_moe%&gt;%\n kable(\n      col.names = c(\"Quality Issues (MOE &gt; %15)\", \"Total Tracts Detected\", \"Average Total Population\", \"Average of White Residents(%)\", \"Average of Black Residents(%)\", \"Average of Hispanic Residents (%)\"),\n      caption = \"Average Demographics Per County\",\n     digits=c(0,0,0,1,1,1)\n)\n\n\nAverage Demographics Per County\n\n\n\n\n\n\n\n\n\n\nQuality Issues (MOE &gt; %15)\nTotal Tracts Detected\nAverage Total Population\nAverage of White Residents(%)\nAverage of Black Residents(%)\nAverage of Hispanic Residents (%)\n\n\n\n\nTRUE\n413\n3190\n75\n14.8\n2.6\n\n\n\n\n\nPattern Analysis: All of the tracts investigated possessed high margins of error and predominantly white communities. This may suggest that demographic data from less diverse communities is more likely to be unreliable."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nOverall Pattern Identification Across all analyses, the smaller the community being surveyed, the less accurate the data would become. Although the counties that were chosen to be further analyzed at the tract level represented a gradient in income estimation reliability, they all shared similar unreliability in demographic data. Allegheny, the county with the smallest margin of error in income estimation, possessed the largest population of the chosen counties while Forest county, the county with the largest margin of error, possessed the smallest population. These counties similarly, however, possessed a disproportionately large number of white residents compare to other racial and ethnic groups and returned high margins of error in their demographic data.\nEquity Assessment The county data here emphasizes that margins of error vary depending on the type of data collected and are influenced by both the absolute size of a population and its proportion within the broader population. More specifically, smaller groups such as counties with small populations and racial or ethnic minorities are more prone to data inaccuracies and thus more vulnerable to bias in algorithmic decision-making. For instance, the tract with the largest hispanic population, the smallest average population across all tracts, had an even larger margin of error. Decisions made based on the this demographic data may vastly misrepresent the true needs of the hispanic population.\nRoot Cause Analysis The misrepresentation of vulnerable populations such as small or spatially uneven populations in data can result in algorithms using inaccurate inputs and thus, inherently reproducing non-representational outputs. Furthermore, the subjective choices made when analyzing data can also skew outputs.\nStrategic Recommendations To mitigate these risks, the Department should identify margins of error and create thorough thresholds for interpretations of margins of errors. Certain thresholds should call for human oversight and require additional social, historical, and spatial explanations."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\ncounty_summary &lt;-county_data%&gt;%\n  mutate(\n    rec_for_algorithm= case_when(\n    reliability == \"High Confidence (&lt;5%)\" ~ \"Safe for algorithmic decisions\",\n   reliability == \"Moderate Confidence (5%-10%)\" ~ \"Use with caution - monitor outcomes\",\n    reliability == \"Low Confidence (&gt;10%)\" ~ \"Requires manual review or additional data\"\n    )\n  )%&gt;%\n  select(county_name, median_incomeE, MOE_PCT, reliability, rec_for_algorithm)%&gt;%\n  arrange(desc(MOE_PCT))%&gt;%\n  kable(\n    col.names = c(\"County Name\", \"Median Income ($)\", \"MOE Percentage(%)\", \"Reliability\", \"Algorithm Recommendation)\"),\n      caption = \"County-level Algorithmic Recommendations\",\n     digits=c(0,0,1,0,0)\n  )\ncounty_summary\n\n\nCounty-level Algorithmic Recommendations\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income ($)\nMOE Percentage(%)\nReliability\nAlgorithm Recommendation)\n\n\n\n\nForest\n46188\n10.0\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nSullivan\n62910\n9.3\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nUnion\n64914\n7.3\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nMontour\n72626\n7.1\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nElk\n61672\n6.6\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nGreene\n66283\n6.4\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nCameron\n46186\n5.6\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nSnyder\n65914\n5.6\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nCarbon\n64538\n5.3\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nWarren\n57925\n5.2\nModerate Confidence (5%-10%)\nUse with caution - monitor outcomes\n\n\nPike\n76416\n4.9\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nWayne\n59240\n4.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nJuniata\n61915\n4.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nMcKean\n57861\n4.7\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nHuntingdon\n61300\n4.7\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nIndiana\n57170\n4.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBedford\n58337\n4.5\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nPotter\n56491\n4.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLycoming\n63437\n4.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nClarion\n58690\n4.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nAdams\n78975\n4.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nFayette\n55579\n4.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nCrawford\n58734\n3.9\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nClinton\n59011\n3.9\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nWyoming\n67968\n3.9\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nColumbia\n59457\n3.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nFulton\n63153\n3.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nMercer\n57353\n3.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nArmstrong\n61011\n3.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBradford\n60650\n3.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBlair\n59386\n3.5\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nVenango\n59278\n3.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nMifflin\n58012\n3.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nJefferson\n56607\n3.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nCambria\n54221\n3.3\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nTioga\n59707\n3.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nMonroe\n80656\n3.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nPerry\n76103\n3.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nSusquehanna\n63968\n3.1\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLawrence\n57585\n3.1\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nFranklin\n71808\n3.0\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nClearfield\n56982\n2.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nSomerset\n57357\n2.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nCentre\n70087\n2.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLebanon\n72532\n2.7\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nNorthumberland\n55952\n2.7\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nButler\n82932\n2.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLackawanna\n63739\n2.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nErie\n59396\n2.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nSchuylkill\n63574\n2.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nWashington\n74403\n2.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLuzerne\n60836\n2.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBeaver\n67194\n2.3\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nDauphin\n71046\n2.3\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nCumberland\n82849\n2.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLehigh\n74973\n2.0\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nWestmoreland\n69454\n2.0\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nNorthampton\n82201\n1.9\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nLancaster\n81458\n1.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nYork\n79183\n1.8\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nChester\n118574\n1.7\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBerks\n74617\n1.6\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nDelaware\n86390\n1.5\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nBucks\n107826\n1.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nPhiladelphia\n57537\n1.4\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nMontgomery\n107441\n1.3\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\nAllegheny\n72537\n1.2\nHigh Confidence (&lt;5%)\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Allegheny, Montgomery, Bucks, Chester, Delaware, Lancaster, York, Northampton, Lehigh, Cumberland, Dauphin, Beaver, Luzerne, Washington, Schuylkill, Erie, Lackawanna, Butler, Lebanon, Centre, Somerset, Clearfield, Franklin, Lawrence, Susquehanna, Perry, Monroe, Tioga, Cambria, Jefferson, Mifflin, Venango, Blair, Bradford, Armstrong, Mercer, Fulton, Columbia, Wyoming, Clinton, Crawford, Fayette, Adams, Clarion, Lycoming, Potter, Bedford, Indiana, Huntingdon, McKean, Juniata, Wayne, Pike\n\nJustification: Low margins of error indicate reduced risk in algorithmic bias\n\nCounties requiring additional oversight: Sullivan, Union, Montour, Elk, Greene, Cameron, Snyder, Carbon, Warren\n\nJustification: Although these counties fall in the moderate confidence range, outcomes should be monitored to ensure appropriateness.\n\nCounties needing alternative approaches: Forest\n\nJustifications: Although Forest county falls in the moderate confidence range, it borders the low confidence range and likely should avoid automated decisions until the quality of data improves."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow small or strict should data reliability ranges be?\nWould it be statistically meaningful to compare margin of error percentages of different variables?"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment_2/Rutherford_Angel_Assignment2.html",
    "href": "assignments/assignment_2/Rutherford_Angel_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\n\n# Set Census API key\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n# Load spatial data\npa_counties &lt;- st_read(\"./data/Pennsylvania_County_Boundaries.shp\")\nhospitals &lt;- st_read(\"./data/hospitals.geojson\")\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n# Check that all data loaded correctly\npa_counties\nhospitals\ncensus_tracts\n\nThere are 223 hospitals and 3445 census tracts in the datasets. The coordinate reference system for each dataset is as follows: WGS 84 / Pseudo-Mercator for county data, WGS 84 for the hospital data, and NAD83 for the census tracts.\n\n\n\n\n\n# Get demographic data from ACS\n\ntract_data &lt;- get_acs(\n  geography=\"tract\",\n  variables= c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nover_65 = c(\"B01001_020\",\"B01001_021\",\"B01001_022\",\"B01001_023\",\n            \"B01001_024\",\"B01001_025\",\"B01001_044\",\"B01001_045\",\n            \"B01001_046\",\"B01001_047\",\"B01001_048\",\"B01001_049\")\n\nelderly_data = get_acs(\n  geography=\"tract\",\n  variables =c(over_65\n),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nover_65_E&lt;- paste0(over_65, \"E\")\n\nelderly_data&lt;- elderly_data%&gt;%\n  mutate(elderly_pop = rowSums(elderly_data[, over_65_E], na.rm = TRUE))\n  \ntotal_elderly &lt;-elderly_data%&gt;%\n  select(GEOID,elderly_pop)\n\ntract_data &lt;-left_join(tract_data, total_elderly, by=\"GEOID\")\n\n# Join to tract boundaries\n\ndata_with_bounds = left_join(census_tracts, tract_data, by=\"GEOID\")\ndata_with_bounds\n\nno_record= sum(is.na(tract_data$median_incomeE))\nno_record\n\nmedian_median= median(tract_data$median_incomeE, na.rm = TRUE)\nmedian_median\n\nThe ACS data being used in this analysis is from 2022. Excluding the 63 tracts with missing income data, the median income across PA census tracts was $70,188.\n\n\n\n\n\n# Filter for vulnerable tracts based on your criteria\ndata_with_bounds &lt;- data_with_bounds%&gt;%\n  mutate(\n    elderly_pct = elderly_pop/total_popE * 100)\n\nvulnerable_data &lt;- data_with_bounds%&gt;%\n  filter(median_incomeE &lt; 36000 & elderly_pct &gt; 20) \n  \n\nvulnerable_data\n\nprint((nrow(vulnerable_data)/nrow(data_with_bounds))*100)\n\nThe income threshold I chose to identify tracts with low median income was a income less than $36,000, approximately half of the median of all tracts. The threshold used to identify large elderly populations was a population over 20% elderly as it would indicate that more than 1/5th of the tract’s population was elderly. These thresholds were defined to serve as measurements of tract vulnerability. Based on these vulnerability thresholds, 49 tracts or 1.4% of Pennsylvania census tracts were considered vulnerable.\n\n\n\n\n\n# Transform to appropriate projected CRS\nvulnerable_data &lt;- st_transform(vulnerable_data,3365)\nhospitals &lt;- st_transform(hospitals, 3365)\n\n# Calculate distance from each tract centroid to nearest hospital\nvulnerable_distance &lt;- vulnerable_data%&gt;%\n  mutate(\n    distance_to_nearest= as.numeric(apply(st_distance(st_centroid(.),hospitals),\n    1, min)\n  ))\n\nvulnerable_distance\n\nmedian_distance= median(vulnerable_distance$distance_to_nearest , na.rm = TRUE)\nmedian_distance\n\nmax_distance = max(vulnerable_distance$distance_to_nearest , na.rm = TRUE)\nmax_distance\n\nThe datasets’ projected coordinate reference system were transformed to 3365, the ideal projection for Pennsylvania and for measuring distance in units and not distorting degrees of longitude and latitude. The average distance to the nearest hospital for vulnerable tracts was approximately 6395 feet. The maximum distance was about 98,596 feet. Only 1 tract, out all the vulnerable tracts, was more than 15 miles from the nearest hospital.\n\n\n\n\n\n# Create underserved variable\nunderserved_tracts &lt;- vulnerable_distance%&gt;%\n  filter(distance_to_nearest &gt; 79200)\n\nunderserved_tracts\n\nprint((nrow(underserved_tracts)/nrow(vulnerable_data))*100)\n\nOnly one singular tract is considered underserved by the condition of being more than 15 miles or 79,200 feet from the nearest hospital. This means that 2% of tracts identified as vulnerable are underserved. Considering that Pennsylvania is overwhelmingly rural, I am surprised that many vulnerable tracts have a hospital within 15 miles.\n\n\n\n\n\n# Spatial join tracts to counties\nvulnerable_distance &lt;- st_transform(vulnerable_distance, st_crs(pa_counties))\n\ntracts_per_county &lt;- vulnerable_distance %&gt;% \n  st_join(pa_counties) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM)\n\n\n# Aggregate statistics by county\ncounty_vulnerability &lt;- tracts_per_county %&gt;%\n  summarise(\n    num_of_tracts = n(),\n    num_of_underserved = sum(distance_to_nearest &gt; 79200, na.rm = TRUE),\n    pct_underserved =  num_of_underserved/num_of_tracts *100,\n    avg_distance = mean(distance_to_nearest, na.rm = TRUE),\n    total_vulnerable = sum(total_popE, na.rm = TRUE)\n  )%&gt;%\n  arrange(desc(avg_distance))\n\nformated_county_vulnerability &lt;- county_vulnerability%&gt;%\n  mutate(\n    total_vulnerable = comma(total_vulnerable),\n    avg_distance = comma(avg_distance),\n    pct_underserved = percent(pct_underserved/100)\n  )\n\nformated_county_vulnerability%&gt;%  \n  select(COUNTY_NAM, num_of_underserved, num_of_tracts, pct_underserved, avg_distance, total_vulnerable)%&gt;%\n    kable(\n      col.names = c(\n\"County Name\", \"Number of Underserved Tracts\",\n\"Total Number of Vulnerable Tracts\", \"Percentage Underserved\", \"Average Distance\", \"Total Vulnerable Population\"),\n     digits=c(0,1,1,1,1))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nNumber of Underserved Tracts\nTotal Number of Vulnerable Tracts\nPercentage Underserved\nAverage Distance\nTotal Vulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100%\n98,596\n1,988\n\n\nNORTHUMBERLAND\n0\n1\n0%\n50,187\n2,350\n\n\nBEAVER\n0\n1\n0%\n21,733\n2,093\n\n\nLEHIGH\n0\n1\n0%\n17,985\n3,974\n\n\nWESTMORELAND\n0\n3\n0%\n16,963\n3,927\n\n\nLUZERNE\n0\n3\n0%\n13,026\n7,931\n\n\nALLEGHENY\n0\n15\n0%\n11,849\n27,204\n\n\nDELAWARE\n0\n1\n0%\n11,338\n2,373\n\n\nMIFFLIN\n0\n1\n0%\n9,779\n2,418\n\n\nYORK\n0\n1\n0%\n8,109\n1,659\n\n\nMERCER\n0\n2\n0%\n5,722\n4,992\n\n\nCRAWFORD\n0\n1\n0%\n4,899\n2,842\n\n\nWAYNE\n0\n1\n0%\n4,419\n4,469\n\n\nFAYETTE\n0\n1\n0%\n3,726\n3,205\n\n\nPHILADELPHIA\n0\n11\n0%\n3,538\n35,060\n\n\nCAMBRIA\n0\n2\n0%\n3,223\n3,241\n\n\nLAWRENCE\n0\n1\n0%\n2,743\n3,893\n\n\nLACKAWANNA\n0\n1\n0%\n2,651\n2,901\n\n\nBLAIR\n0\n1\n0%\n2,192\n1,436\n\n\nERIE\n0\n2\n0%\n1,238\n3,070\n\n\n\n\n\nOnly 1 county, Cameron county, has 100% underserved vulnerable tracts. Allegheny county has the most vulnerable people, 27,204, living far away, about 11,849 feet, from hospitals. There are no patterns in underserved counties but there is a pattern in vulnerability as most vulnerable counties have vulnerable populations less than 5,000, indicating counties with smaller overall population.\n\n\n\n\n\n# Create and format priority counties table\ntop_10_counties &lt;- formated_county_vulnerability %&gt;%\n  slice_head(n=10)\n\n# Format as table with kable() - include appropriate column names and caption \ntop_10_counties %&gt;%\n  select(COUNTY_NAM, pct_underserved, avg_distance, total_vulnerable)%&gt;%\n    kable(\n      col.names = c(\n\"County Name\", \n\"Percentage of Tracts Lacking Adequate Access to Hospitals (over 15 miles to nearest)\",\n\"Average Distance to Nearest Hospital (ft)\",\n\"Total Population of Vulnerable Tracts (tracts with 20% over the age of 65 and a median income less than 3600)\"),\n      caption = \"Top 10 Pennslyvania Counties to Prioritize Healthcare Investments Based on Vulnerability and Access\",\n     digits=c(0,1,1,1)\n)\n\n\nTop 10 Pennslyvania Counties to Prioritize Healthcare Investments Based on Vulnerability and Access\n\n\n\n\n\n\n\n\nCounty Name\nPercentage of Tracts Lacking Adequate Access to Hospitals (over 15 miles to nearest)\nAverage Distance to Nearest Hospital (ft)\nTotal Population of Vulnerable Tracts (tracts with 20% over the age of 65 and a median income less than 3600)\n\n\n\n\nCAMERON\n100%\n98,596\n1,988\n\n\nNORTHUMBERLAND\n0%\n50,187\n2,350\n\n\nBEAVER\n0%\n21,733\n2,093\n\n\nLEHIGH\n0%\n17,985\n3,974\n\n\nWESTMORELAND\n0%\n16,963\n3,927\n\n\nLUZERNE\n0%\n13,026\n7,931\n\n\nALLEGHENY\n0%\n11,849\n27,204\n\n\nDELAWARE\n0%\n11,338\n2,373\n\n\nMIFFLIN\n0%\n9,779\n2,418\n\n\nYORK\n0%\n8,109\n1,659\n\n\n\n\n\nDue to limited data availability for underserved counties, rankings were based on the ten counties with the highest average distance."
  },
  {
    "objectID": "assignments/assignment_2/Rutherford_Angel_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/Rutherford_Angel_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\n\n# Set Census API key\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n# Load spatial data\npa_counties &lt;- st_read(\"./data/Pennsylvania_County_Boundaries.shp\")\nhospitals &lt;- st_read(\"./data/hospitals.geojson\")\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n# Check that all data loaded correctly\npa_counties\nhospitals\ncensus_tracts\n\nThere are 223 hospitals and 3445 census tracts in the datasets. The coordinate reference system for each dataset is as follows: WGS 84 / Pseudo-Mercator for county data, WGS 84 for the hospital data, and NAD83 for the census tracts.\n\n\n\n\n\n# Get demographic data from ACS\n\ntract_data &lt;- get_acs(\n  geography=\"tract\",\n  variables= c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nover_65 = c(\"B01001_020\",\"B01001_021\",\"B01001_022\",\"B01001_023\",\n            \"B01001_024\",\"B01001_025\",\"B01001_044\",\"B01001_045\",\n            \"B01001_046\",\"B01001_047\",\"B01001_048\",\"B01001_049\")\n\nelderly_data = get_acs(\n  geography=\"tract\",\n  variables =c(over_65\n),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nover_65_E&lt;- paste0(over_65, \"E\")\n\nelderly_data&lt;- elderly_data%&gt;%\n  mutate(elderly_pop = rowSums(elderly_data[, over_65_E], na.rm = TRUE))\n  \ntotal_elderly &lt;-elderly_data%&gt;%\n  select(GEOID,elderly_pop)\n\ntract_data &lt;-left_join(tract_data, total_elderly, by=\"GEOID\")\n\n# Join to tract boundaries\n\ndata_with_bounds = left_join(census_tracts, tract_data, by=\"GEOID\")\ndata_with_bounds\n\nno_record= sum(is.na(tract_data$median_incomeE))\nno_record\n\nmedian_median= median(tract_data$median_incomeE, na.rm = TRUE)\nmedian_median\n\nThe ACS data being used in this analysis is from 2022. Excluding the 63 tracts with missing income data, the median income across PA census tracts was $70,188.\n\n\n\n\n\n# Filter for vulnerable tracts based on your criteria\ndata_with_bounds &lt;- data_with_bounds%&gt;%\n  mutate(\n    elderly_pct = elderly_pop/total_popE * 100)\n\nvulnerable_data &lt;- data_with_bounds%&gt;%\n  filter(median_incomeE &lt; 36000 & elderly_pct &gt; 20) \n  \n\nvulnerable_data\n\nprint((nrow(vulnerable_data)/nrow(data_with_bounds))*100)\n\nThe income threshold I chose to identify tracts with low median income was a income less than $36,000, approximately half of the median of all tracts. The threshold used to identify large elderly populations was a population over 20% elderly as it would indicate that more than 1/5th of the tract’s population was elderly. These thresholds were defined to serve as measurements of tract vulnerability. Based on these vulnerability thresholds, 49 tracts or 1.4% of Pennsylvania census tracts were considered vulnerable.\n\n\n\n\n\n# Transform to appropriate projected CRS\nvulnerable_data &lt;- st_transform(vulnerable_data,3365)\nhospitals &lt;- st_transform(hospitals, 3365)\n\n# Calculate distance from each tract centroid to nearest hospital\nvulnerable_distance &lt;- vulnerable_data%&gt;%\n  mutate(\n    distance_to_nearest= as.numeric(apply(st_distance(st_centroid(.),hospitals),\n    1, min)\n  ))\n\nvulnerable_distance\n\nmedian_distance= median(vulnerable_distance$distance_to_nearest , na.rm = TRUE)\nmedian_distance\n\nmax_distance = max(vulnerable_distance$distance_to_nearest , na.rm = TRUE)\nmax_distance\n\nThe datasets’ projected coordinate reference system were transformed to 3365, the ideal projection for Pennsylvania and for measuring distance in units and not distorting degrees of longitude and latitude. The average distance to the nearest hospital for vulnerable tracts was approximately 6395 feet. The maximum distance was about 98,596 feet. Only 1 tract, out all the vulnerable tracts, was more than 15 miles from the nearest hospital.\n\n\n\n\n\n# Create underserved variable\nunderserved_tracts &lt;- vulnerable_distance%&gt;%\n  filter(distance_to_nearest &gt; 79200)\n\nunderserved_tracts\n\nprint((nrow(underserved_tracts)/nrow(vulnerable_data))*100)\n\nOnly one singular tract is considered underserved by the condition of being more than 15 miles or 79,200 feet from the nearest hospital. This means that 2% of tracts identified as vulnerable are underserved. Considering that Pennsylvania is overwhelmingly rural, I am surprised that many vulnerable tracts have a hospital within 15 miles.\n\n\n\n\n\n# Spatial join tracts to counties\nvulnerable_distance &lt;- st_transform(vulnerable_distance, st_crs(pa_counties))\n\ntracts_per_county &lt;- vulnerable_distance %&gt;% \n  st_join(pa_counties) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM)\n\n\n# Aggregate statistics by county\ncounty_vulnerability &lt;- tracts_per_county %&gt;%\n  summarise(\n    num_of_tracts = n(),\n    num_of_underserved = sum(distance_to_nearest &gt; 79200, na.rm = TRUE),\n    pct_underserved =  num_of_underserved/num_of_tracts *100,\n    avg_distance = mean(distance_to_nearest, na.rm = TRUE),\n    total_vulnerable = sum(total_popE, na.rm = TRUE)\n  )%&gt;%\n  arrange(desc(avg_distance))\n\nformated_county_vulnerability &lt;- county_vulnerability%&gt;%\n  mutate(\n    total_vulnerable = comma(total_vulnerable),\n    avg_distance = comma(avg_distance),\n    pct_underserved = percent(pct_underserved/100)\n  )\n\nformated_county_vulnerability%&gt;%  \n  select(COUNTY_NAM, num_of_underserved, num_of_tracts, pct_underserved, avg_distance, total_vulnerable)%&gt;%\n    kable(\n      col.names = c(\n\"County Name\", \"Number of Underserved Tracts\",\n\"Total Number of Vulnerable Tracts\", \"Percentage Underserved\", \"Average Distance\", \"Total Vulnerable Population\"),\n     digits=c(0,1,1,1,1))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nNumber of Underserved Tracts\nTotal Number of Vulnerable Tracts\nPercentage Underserved\nAverage Distance\nTotal Vulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100%\n98,596\n1,988\n\n\nNORTHUMBERLAND\n0\n1\n0%\n50,187\n2,350\n\n\nBEAVER\n0\n1\n0%\n21,733\n2,093\n\n\nLEHIGH\n0\n1\n0%\n17,985\n3,974\n\n\nWESTMORELAND\n0\n3\n0%\n16,963\n3,927\n\n\nLUZERNE\n0\n3\n0%\n13,026\n7,931\n\n\nALLEGHENY\n0\n15\n0%\n11,849\n27,204\n\n\nDELAWARE\n0\n1\n0%\n11,338\n2,373\n\n\nMIFFLIN\n0\n1\n0%\n9,779\n2,418\n\n\nYORK\n0\n1\n0%\n8,109\n1,659\n\n\nMERCER\n0\n2\n0%\n5,722\n4,992\n\n\nCRAWFORD\n0\n1\n0%\n4,899\n2,842\n\n\nWAYNE\n0\n1\n0%\n4,419\n4,469\n\n\nFAYETTE\n0\n1\n0%\n3,726\n3,205\n\n\nPHILADELPHIA\n0\n11\n0%\n3,538\n35,060\n\n\nCAMBRIA\n0\n2\n0%\n3,223\n3,241\n\n\nLAWRENCE\n0\n1\n0%\n2,743\n3,893\n\n\nLACKAWANNA\n0\n1\n0%\n2,651\n2,901\n\n\nBLAIR\n0\n1\n0%\n2,192\n1,436\n\n\nERIE\n0\n2\n0%\n1,238\n3,070\n\n\n\n\n\nOnly 1 county, Cameron county, has 100% underserved vulnerable tracts. Allegheny county has the most vulnerable people, 27,204, living far away, about 11,849 feet, from hospitals. There are no patterns in underserved counties but there is a pattern in vulnerability as most vulnerable counties have vulnerable populations less than 5,000, indicating counties with smaller overall population.\n\n\n\n\n\n# Create and format priority counties table\ntop_10_counties &lt;- formated_county_vulnerability %&gt;%\n  slice_head(n=10)\n\n# Format as table with kable() - include appropriate column names and caption \ntop_10_counties %&gt;%\n  select(COUNTY_NAM, pct_underserved, avg_distance, total_vulnerable)%&gt;%\n    kable(\n      col.names = c(\n\"County Name\", \n\"Percentage of Tracts Lacking Adequate Access to Hospitals (over 15 miles to nearest)\",\n\"Average Distance to Nearest Hospital (ft)\",\n\"Total Population of Vulnerable Tracts (tracts with 20% over the age of 65 and a median income less than 3600)\"),\n      caption = \"Top 10 Pennslyvania Counties to Prioritize Healthcare Investments Based on Vulnerability and Access\",\n     digits=c(0,1,1,1)\n)\n\n\nTop 10 Pennslyvania Counties to Prioritize Healthcare Investments Based on Vulnerability and Access\n\n\n\n\n\n\n\n\nCounty Name\nPercentage of Tracts Lacking Adequate Access to Hospitals (over 15 miles to nearest)\nAverage Distance to Nearest Hospital (ft)\nTotal Population of Vulnerable Tracts (tracts with 20% over the age of 65 and a median income less than 3600)\n\n\n\n\nCAMERON\n100%\n98,596\n1,988\n\n\nNORTHUMBERLAND\n0%\n50,187\n2,350\n\n\nBEAVER\n0%\n21,733\n2,093\n\n\nLEHIGH\n0%\n17,985\n3,974\n\n\nWESTMORELAND\n0%\n16,963\n3,927\n\n\nLUZERNE\n0%\n13,026\n7,931\n\n\nALLEGHENY\n0%\n11,849\n27,204\n\n\nDELAWARE\n0%\n11,338\n2,373\n\n\nMIFFLIN\n0%\n9,779\n2,418\n\n\nYORK\n0%\n8,109\n1,659\n\n\n\n\n\nDue to limited data availability for underserved counties, rankings were based on the ten counties with the highest average distance."
  },
  {
    "objectID": "assignments/assignment_2/Rutherford_Angel_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/Rutherford_Angel_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\n# Create county-level access map\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncounties_with_demographics &lt;- pa_counties %&gt;%\n  left_join(county_vulnerability, by = \"COUNTY_NAM\")\n\nggplot(counties_with_demographics) +\n  geom_sf(aes(fill = pct_underserved), color = \"black\", size = 0.5) +\n  geom_sf(data = hospitals, color = \"red\", size = 2) +\n  scale_fill_viridis_c(\n    na.value = \"white\",\n    name = \"Percentage Underserved\",\n    labels = scales::label_number(suffix = \"%\"),\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"Pennsylvania Counties with Vulnerable Tracts\\nand their Percentage of Underserved Vulnerable Tracts\",\n    subtitle = \"Red = Hospitals\",\n    caption = \"Source: ACS 2022\"\n  ) +\n  theme_void()+\n  theme(\n  plot.title = element_text(face = \"bold\", hjust =0))\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\nhospitals_with_counties &lt;- hospitals %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\n\ntarget_county &lt;- pa_counties %&gt;%\n  filter(FIPS_COUNT == underserved_tracts$COUNTYFP)\n\nmy_neighbors &lt;- pa_counties %&gt;%\n  st_filter(target_county, .predicate = st_touches)\n\n\nclipped_neighbor_tracts &lt;- st_intersection(census_tracts, my_neighbors)\nclipped_target_tracts &lt;- st_intersection(census_tracts, target_county)\n\nneighbor_hospitals &lt;- hospitals_with_counties %&gt;%\n  filter(COUNTY_NAM %in% my_neighbors$COUNTY_NAM)\n\nggplot()+\n  geom_sf(data = clipped_neighbor_tracts, color=\"black\")+\n  geom_sf(data = clipped_target_tracts, color=\"black\")+\n  geom_sf(data = target_county, fill=\"lightgray\", color=\"black\", alpha=0.7)+\n  geom_sf(data = my_neighbors, fill=\"white\", color=\"black\", alpha=0.7)+\n  geom_sf(data = underserved_tracts, fill= \"blue\")+\n  geom_sf(data = neighbor_hospitals, color=\"red\", size=2)+\n  labs(\n    title = \"Surrounding Pennsylvania Counties and Tracts of the Underserved Tract\",\n    subtitle = \"Red = Hospitals, Blue = Underserved Tract\",\n    caption = \"Source: ACS 2022\")+\n  theme_void()+\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis\n\n# Create distribution visualization\n\nggplot(county_vulnerability, aes(x = COUNTY_NAM, y = avg_distance)) +\n  geom_col(fill = \"blue\", color = \"black\") +\n  labs(title = \"Bar Plot Average Distance to Hospitals Across Vulnerable Tracts By County\",\n       x = \"County Name\", \n       y = \"Average Distance (feet)\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n  panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nThis bar chart shows the average distance to hospitals across vulnerable tracts by Pennsylvania counties. Cameron County stands out dramatically with the highest value, suggesting that residents there may face significant challenges accessing hospital care. Most other counties have relatively low averages, indicating better proximity to healthcare facilities.\n\nPer previous submission feedback, I removed instructional text from the final version of my assignment. Additionally, I omitted tables and outputs that were generated for my personal reference."
  },
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#which-features-matter-most",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#which-features-matter-most",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Which Features Matter Most?",
    "text": "Which Features Matter Most?\n\n\n\n\n\n\n\n\nFeature\nDirection\nInterpretation\n\n\n\n\nLiving area\n↑\nStrongest driver of housing price\n\n\nAge + Age²\n↓ then ↑\nU-shaped pattern — older historic homes regain value\n\n\nExterior good\n↑\nMaintenance condition positively impacts price\n\n\nMedian income / Education\n↑\nSocioeconomic context drives demand\n\n\nPoverty rate / Crime\n↓\nNegative neighborhood effects\n\n\nInteraction: Living area × Wealthy neighborhood\n↓\nLarger homes add less"
  },
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#thank-you-for-listening",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#thank-you-for-listening",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank you for listening",
    "text": "Thank you for listening\nAny questions?"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  }
]