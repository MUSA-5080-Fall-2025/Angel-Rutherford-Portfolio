[
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Appendix.html",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Phase 1: Data Preparation\nIn this analysis, we attempt to measure what factors we can use to predict the sale prices of residential properties in Philadelphia. In order to prepare the dataset for modeling residential property sales, we applied a series of targeted cleaning and structural, socioeconomic, and spatial variable selection.\nWe filtered our property sales data to only show sales for the past two years to ensure up-to-date insights. We restricted the dataset to \"SINGLE FAMILY\" and \"MULTI FAMILY\" homes, excluding apartments and non-residential sales to ensure comparability, relevancy, and accuracy in our prediction models. Various apartments contained within the same building were found to be listed with a comprehensive sales price of the entire building which posed the threat of distorting our analysis. Observation values that were regarded as data entry errors or outliers were also removed. Properties with zero bathrooms, bedrooms, or livable area were removed as well as properties with a sales price below $10,000 and above $1,000,000.\nWe retained select structural attributes from our original dataset that we considered theoretically relevant to predicting sale price: number of bedrooms, number of bathrooms, total livable area, year the home was built, exterior condition, availability of garage spaces, and finally, the dependent variable, sale price.\nCode for Property Sales Data Cleaning and Variable Selection\n\n\nCode\n#load necessary libraries\nlibrary(modelsummary)\nlibrary(stargazer)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(caret)\nlibrary(knitr)\nlibrary(scales)\n\n#load census key for later use\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n#save data in url \nurl &lt;- \"https://phl.carto.com/api/v2/sql?filename=opa_properties_public&format=geojson&skipfields=cartodb_id&q=SELECT+*+FROM+opa_properties_public\"\n\n#suppress warnings for clarity and read data as spatial object\nsuppressWarnings({\n property_data &lt;- st_read(url)\n})\n\n#clean data\nparcel_data &lt;- property_data%&gt;%\n  select(location, #load columns that could potentially be used as predictors \n         category_code_description, #maybe garage_spaces and central air too?\n         number_of_bedrooms,\n         number_of_bathrooms, \n         total_livable_area,\n         year_built,\n         exterior_condition,\n         garage_spaces,\n         sale_price,\n         sale_date)%&gt;%\n  filter(category_code_description %in% \n  c(\"SINGLE FAMILY\",\"MULTI FAMILY\"))%&gt;% #no apartments, sales price of building\n  drop_na(number_of_bedrooms, #remove anomalies like houses with no rooms\n          number_of_bathrooms,\n          total_livable_area,\n          sale_price,\n          year_built) %&gt;%\n  filter(number_of_bedrooms&gt;0, \n         number_of_bathrooms&gt;0,\n         total_livable_area&gt;0, \n         sale_price&gt;=10000, sale_price &lt;=1000000)%&gt;% #remove very low/high prices\n  mutate(sale_year = str_remove(sale_date, \"-.*\"))%&gt;%   \n  filter(sale_year %in% c(\"2023\",\"2024\"))%&gt;% #limit to only 2023 and 2024\n  mutate(year_built = as.numeric(year_built))%&gt;%\n  mutate(Age = 2025 - year_built)%&gt;% filter(Age &lt;2000)%&gt;% #create a age column\n  filter(exterior_condition != 0) %&gt;% #create exterior condition binary  \n  mutate(\n    exterior_good = case_when(\n      exterior_condition &gt;= 1 & exterior_condition &lt;= 5 ~ 1,\n      exterior_condition &gt;= 6 & exterior_condition &lt;= 9 ~ 0,\n      TRUE ~ NA_real_ \n    )\n  )\n\n\n\n\n\nTable 1. Property Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nRaw Property Data\n583824\n79\n\n\nCleaned Parcel Data\n25268\n14\n\n\n\n\n\nWe derived our socioeconomic predictors from tract-level census data provided by the 2022 American Community Survey: median income, number with at least a bachelor’s degree, total number of those with at least a bachelor’s degree, number of those living in poverty, and total of those living in poverty. Census tracts with missing median income or zero reported as the median income were removed as it often indicated missing or zero values for other key predictors. We also mutated our census dataset to include two more columns, the percentage of people with bachelors and percentage of people in poverty, in order to standardize the observations across varying tract population sizes. Spatial data of the census tracts was also loaded in order to join our census variables to our parcel-level property data.\nCode for Census Data Cleaning and Variable Selection\n\n\nCode\n#load data about poverty(counts and total), bachelors(counts and total), and income \ncensus_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = c(\n    median_income = \"B19013_001\",\n    num_with_bach = \"B15003_022\",\n    bachelors_total =\"B15003_001\",\n    num_in_poverty = \"B17001_002\",\n    poverty_total =\"B17001_001\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n#create percentage columns for bachelors and poverty \nphilly_census &lt;- census_data%&gt;%\n  mutate(\n    percentage_bach = num_with_bachE / bachelors_totalE,\n    percentage_pov =  num_in_povertyE / poverty_totalE\n  )\n\n#remove data errors or incomplete fields \nphilly_census &lt;- philly_census%&gt;%\n  drop_na(median_incomeE)%&gt;%\n  filter(median_incomeE&gt;0)\n\n\n#spatial census data \nphiladelphia_tracts &lt;- tracts(\n  state = \"PA\",\n  county = \"Philadelphia\",\n  cb = TRUE,\n  year = 2022\n)\n\n#join census data and tract geometry to PARCEL data\nparcel_data &lt;- parcel_data %&gt;%\n  st_transform(st_crs(philadelphia_tracts))%&gt;%\n  st_join(philadelphia_tracts, join = st_within)%&gt;%\n  left_join(philly_census, by = \"GEOID\")\n\n\n\n\n\nTable 2. Census Dataset Dimensions Before and After Cleaning and Selecting Varaiables\n\n\nDataset\nRows\nColumns\n\n\n\n\nCensus Data\n408\n12\n\n\nCleaned Census Data\n383\n14\n\n\n\n\n\nTo further contextualize house prices, we loaded spatial datasets including the locations of colleges and universities, 2024 crime incidents, and Philadelphia neighborhood boundaries. These datasets were loaded with the intention of engineering spatial features such as proximity measurements and neighborhood stratification in order to account for spatial patterns and potential interactive spatial effects within our predictive model. Only the 2024 crime incidents needed to be cleaned due to the missing geometries that would prevent spatial analysis.\nCode for Spatial Data Cleaning and Variable Selection\n\n\nCode\n#load university data\nuniversity_data &lt;- st_read(\"./data/Universities_Colleges.geojson\")\n\n#load 2024 crime incident data\norg_crime_data &lt;-st_read(\"./data/incidents_part1_part2.shp\")\n\n#removed crime incidents with no geometry \ncrime_data &lt;- org_crime_data %&gt;%\n  filter(!st_is_empty(geometry))\n\n#load neighborhood data \nneighborhoods &lt;- st_read(\"./data/philadelphia-neighborhoods.shp\")\n\n\n\n\n\nTable 3. Spatial Datasets Dimensions Before and After Cleaning\n\n\nDataset\nRows\nColumns\n\n\n\n\nCrime Data\n160388\n14\n\n\nCleaned Crime Data\n153644\n14\n\n\n\n\n\n\n\nPhase 2: Exploratory Data Analysis\nWe created a histogram to visualize the distribution of home sale prices in Philadelphia. The resulting graph revealed a positively skewed distribution, indicating that most properties were sold at lower price points, primarily between $150,000 and $350,000, while a small number of high-value homes sold for up to $1,000,000. As it pertains to our predictive model, this skewness suggests that a log-transformation of sale prices may be necessary to normalize the distribution of values and potentially improve model performance. It also indicates the need for diagnostic checks to determine the impact of outliers in order to ensure that the few high-priced homes do not distort our model estimates.\n\n\n\n\n\n\n\n\n\nWe also explored the geographic distribution of sales prices across the neighborhoods in Philadelphia. We calculated the median sales price by each Philadelphia neighborhood. The map in Figure 2 shows distinct spatial patterns with higher prices above $400,000, represented by lighter colors, being concentrated in neighborhoods such as Center City, University City, and parts of Northwest Philadelphia.\n\n\n\n\n\n\n\n\n\nOne interesting relationship we found when exploring the structural predictors of sales price was the non-linear association between home age and sale price. Because we have already determined that sale prices are not normally distributed, we plotted our predictors by the log-transformed sale price to limit distortion when attempting to assess the relationship between predictors and sales price. Even with this transformation, Figure 3 shows that sale prices tend to decline for middle-aged homes but rebound for older, more historic properties. The figure includes both a linear and quadratic line to further emphasize that a linear relationship is not sufficient in capturing the impact of age on house prices.\n\n\n\n\n\n\n\n\n\nOne spatial factor that we found theoretically relevant to predicting sales price was proximity to violent crimes. We hypothesized that a home’s proximity to violent crimes such as homicides, aggravated assaults with and without firearms, and armed robberies may lead to a depreciation in house value. To measure this, loaded the location of crime incidents 2024 and calculated the density of crime within 600 feet of each home. In Figure 4 we explored this relationship between violent crimes and found a general negative trend between proximity to crime and sale price.\n\n\n\n\n\n\n\n\n\nWe also created a violin plot to illustrates the distribution and density of sale prices across within the top ten neighborhoods with the highest median sale prices. The violin plot in Figure 5 reveals heterogeneity in sale prices within the wealthiest neighborhoods, indicating juxtaposition of high and low sales prices within close proximity of one another. While some neighborhoods are more concentrated at similar sales prices, neighborhoods like Bella Vista and Northern Liberties had long tails extending towards the lowest sale prices.\n\n\n\n\n\n\n\n\n\n\n\nPhase 3: Feature Engineering\nIn order to account for the spatial patterns highlight during the exploratory phase, we created a number of spatial features to incorporate into our predictive model.\nCode for Feature Engineering: Crime Buffer\n\n\nCode\n#set crs for distance calculations\ncrime_proj &lt;- st_transform(crime_data, 3365)\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nuniversity_proj &lt;- st_transform(university_data, 3365)\n\n#proximity (within 600 feet) to violent crime\n\nparcel_buffers&lt;- st_buffer(parcel_proj, dist=600)\n\nviolent_crimes &lt;- c(\"Homicide - Criminal\",\"Aggravated Assault No Firearm\",\"Robbery Firearm\",\"Aggravated Assault Firearm\")\n\nviolent_proj &lt;- crime_proj %&gt;%\n  filter(text_gener %in% violent_crimes)\n    \nviolent_crime_counts &lt;- st_intersects(parcel_buffers, violent_proj)\nviolent_crime_counts &lt;- lengths(violent_crime_counts)\nparcel_data$violent_crime_600ft &lt;- violent_crime_counts\n\n\nCode for Feature Engineering: K-Nearest Neighbor for Colleges\n\n\nCode\n#knn to university \n\nuniversity_proj &lt;- st_transform(university_data, 3365) #projection changed in order to calculate distance \n\n\ndist_matrix &lt;- st_distance(parcel_proj, university_proj)\n\n\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\nparcel_data$college_nn1 &lt;- get_knn_distance(dist_matrix, k = 1)\nparcel_data$college_nn3 &lt;- get_knn_distance(dist_matrix, k = 3)\nparcel_data$college_nn5 &lt;- get_knn_distance(dist_matrix, k = 5)\n\n#determine which nearest neighbor is correlated the most with sales price \n\nparcel_data %&gt;% \n  st_drop_geometry() %&gt;%\n  select(sale_price, college_nn1, college_nn3, college_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(sale_price)\n\n\nCode for Feature Engineering: Neighborhood Interaction Effects\n\n\nCode\n# wealthy neighborhood interaction effects\nparcel_proj &lt;- st_transform(parcel_data, 3365)\nneighborhood_proj &lt;-st_transform(neighborhoods, 3365)\n\n\n#median sales price \nwealthy_neighborhoods &lt;- parcel_with_neighborhood%&gt;%\n  filter(median_price &gt;= 275500)%&gt;%\n  pull(NAME)\n\n#finding the parcels that fall within neighborhood boundaries \nparcel_data &lt;- parcel_proj %&gt;%\n  st_join(neighborhood_proj, join = st_within) %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(NAME %in% wealthy_neighborhoods, \"Wealthy\", \"Not Wealthy\"), #creating a flag to delineate them into two categories \n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n\n\n\n\nTable 4. Summary of Engineered Features\n\n\n\n\n\n\n\n\nFeature_Name\nFeature_Type\nDescription\nJustification\n\n\n\n\nviolent_crime_600ft\nBuffer-based (spatial)\nCount of violent crimes within 600 ft of parcel\nCaptures neighborhood safety and crime exposure\n\n\ncollege_nn1\nkNN (spatial distance)\nAverage distance to nearest college/university (k=1)\nMeasures proximity to educational amenities\n\n\ncollege_nn3\nkNN (spatial distance)\nAverage distance to 3 nearest colleges/universities\nCaptures local educational accessibility\n\n\ncollege_nn5\nkNN (spatial distance)\nAverage distance to 5 nearest colleges/universities\nCaptures broader proximity to higher education hubs\n\n\nmedian_incomeE\nCensus (socioeconomic)\nMedian household income of census tract\nRepresents economic conditions of local area\n\n\npercentage_bach\nCensus (education)\nPercent of tract population with bachelor’s degree\nEducation level could be connected to property values\n\n\npercentage_pov\nCensus (poverty)\nPercent of tract population below poverty line\nReflects socioeconomic disadvantage\n\n\nwealthy_neighborhood\nInteraction term (categorical)\nIndicates parcels located in wealthy neighborhoods\nIdentifies context of neighborhood wealth levels\n\n\nlog_sale_price\nTransformation (continuous)\nNatural log of sale price\nStabilizes variance for regression modeling\n\n\nlog_livable_area\nTransformation (continuous)\nNatural log of total livable area (sq ft)\nNormalizes skewed size variable for modeling\n\n\nAge\nStructural (numeric)\nYears since building construction\nAccounts for devaluation of property value with time\n\n\nexterior_good\nCondition (binary)\n1 = good/average exterior, 0 = poor\nChanged categorical condition to numeric to understand exterior desirability\n\n\n\n\n\nThe features included were engineered to capture social, economic, and environmental factors influencing housing prices. Buffer-based and nearest-neighbor measures quantify local accessibility and safety. Census data enrich parcels with socio-economic context. Interaction terms account for neighborhood-level wealth effects which helps contextualize external features of a home, for instance homes in a wealthy neighborhood are more likely to have a higher property value than those not in a wealthy neighborhood. Applying a binary indicator improves model interpretability and performance.\n\n\nPhase 4: Model Building\nWhen building our model’s be progressively built out models to expand upon the last with out first model being the baseline of all our models and using only structural variables, the second model adding socioeconomic census variables, our third model introducing our spatial features, and our fourth model added spatial interactions and fixed effects.\nCode for Model Building\n\n\nCode\n#create log livable area for modeling\n\nparcel_data &lt;- parcel_data%&gt;%\n  mutate(log_livable_area = log(total_livable_area))\n\n#structural features only \nmodel1 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area + garage_spaces + Age + I(Age^2) + exterior_good, data = parcel_data)\n\n\n#structural + census \nmodel2 &lt;- lm(log_sale_price ~ number_of_bathrooms + number_of_bedrooms + log_livable_area  + garage_spaces + Age + I(Age^2) + exterior_good + median_incomeE + percentage_bach + percentage_pov, data = parcel_data)\n\n#structural + census + spatial \nmodel3 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\n\n#structural + census + spatial + interactions \nmodel4 &lt;- lm(log_sale_price ~ number_of_bathrooms+ garage_spaces + Age + I(Age^2) + exterior_good + log_livable_area * wealthy_neighborhood + median_incomeE +percentage_bach + percentage_pov + college_nn1 + violent_crime_600ft, data = parcel_data)\n\n\nOur summary table of coefficients for predictors by model indicate statistical numerically as well as with stars to account for low values omitted through the rounding of coefficients.\nFull Model Summary of Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n5.512***\n7.275***\n7.615***\n7.175***\n\n\n\n[5.254, 5.771]\n[7.060, 7.491]\n[7.429, 7.802]\n[6.933, 7.418]\n\n\nnumber_of_bathrooms\n0.283***\n0.215***\n0.213***\n0.209***\n\n\n\n[0.269, 0.296]\n[0.204, 0.226]\n[0.202, 0.224]\n[0.199, 0.220]\n\n\nnumber_of_bedrooms\n-0.192***\n-0.013*\n\n\n\n\n\n[-0.204, -0.180]\n[-0.023, -0.003]\n\n\n\n\nlog_livable_area\n0.874***\n0.497***\n0.461***\n0.515***\n\n\n\n[0.838, 0.910]\n[0.467, 0.528]\n[0.437, 0.484]\n[0.483, 0.548]\n\n\ngarage_spaces\n0.104***\n0.098***\n0.050***\n0.051***\n\n\n\n[0.089, 0.119]\n[0.085, 0.110]\n[0.037, 0.062]\n[0.039, 0.064]\n\n\nAge\n-0.012***\n-0.004***\n-0.004***\n-0.003***\n\n\n\n[-0.013, -0.011]\n[-0.004, -0.003]\n[-0.005, -0.003]\n[-0.004, -0.003]\n\n\nI(Age^2)\n0.000***\n0.000***\n0.000***\n0.000***\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n\n\nexterior_good\n1.180***\n0.941***\n0.881***\n0.877***\n\n\n\n[1.094, 1.265]\n[0.871, 1.011]\n[0.813, 0.950]\n[0.809, 0.944]\n\n\nmedian_incomeE\n\n0.000***\n0.000***\n0.000***\n\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n[0.000, 0.000]\n\n\npercentage_bach\n\n1.564***\n1.464***\n1.051***\n\n\n\n\n[1.474, 1.655]\n[1.373, 1.555]\n[0.956, 1.147]\n\n\npercentage_pov\n\n-0.862***\n-0.476***\n-0.375***\n\n\n\n\n[-0.933, -0.790]\n[-0.549, -0.403]\n[-0.447, -0.302]\n\n\ncollege_nn1\n\n\n0.000***\n0.000***\n\n\n\n\n\n[0.000, 0.000]\n[0.000, 0.000]\n\n\nviolent_crime_600ft\n\n\n-0.020***\n-0.018***\n\n\n\n\n\n[-0.021, -0.019]\n[-0.019, -0.017]\n\n\nwealthy_neighborhoodWealthy\n\n\n\n1.093***\n\n\n\n\n\n\n[0.815, 1.372]\n\n\nlog_livable_area × wealthy_neighborhoodWealthy\n\n\n\n-0.118***\n\n\n\n\n\n\n[-0.157, -0.079]\n\n\nNum.Obs.\n24556\n24453\n24453\n24453\n\n\nR2\n0.351\n0.569\n0.589\n0.600\n\n\nR2 Adj.\n0.350\n0.569\n0.589\n0.600\n\n\nF\n1892.735\n3226.742\n3189.486\n2821.134\n\n\nRMSE\n0.61\n0.50\n0.48\n0.48\n\n\n\n\np &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\nAcross all models, the coefficient for the structural predictors like the number of bathrooms, garage spaces, livable area, and exterior conditions remains consistently positive and statistically significant, with livable area and exterior conditions being the most influential of the predictors. The coefficients for age and it’s quadratic adjustment showed that older homes sold for less while the coefficients for the number of bedrooms showed that it became less statistically significant as non-structural predictors were introduced.\nAs census variables are introduced in Model 2, median income, educational attainment, and poverty rate all show expected relationships: higher income and education levels are associated with higher sale prices, while higher poverty rates are negatively associated.Our spatial features introduced in Model 3, distance from the nearest college and density of crime were also identified as statistically significant. In Model 4, the coefficient for wealthy neighborhood is large and positive. There is, however, a slight negative interaction between log(livable area) and wealthy neighborhoods. This suggests that while larger homes still tend to sell for more, the appreciation of value of each additional square foot is less pronounced in already expensive areas.\nOverall, the progression from Model 1 to Model 4 shows increasing explanatory power (R² rising from 0.35 to 0.60), and decreasing RMSE, indicating improved model fit as more structural and contextual variables are added.\n\n\nPhase 5: Model Validation\nIn order to further assess the predictive power of our models, we performed 10-fold cross validation for each model in order to determine how well the model predicts for unseen data.\nCode for 10-fold Cross-Validation\n\n\nCode\nparcel_data &lt;- na.omit(parcel_data)\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 10  # intiate 10-fold CV\n)\n\nmodel_cv1 &lt;-  train(log_sale_price ~ \n                      number_of_bathrooms +\n                      number_of_bedrooms +\n                      log_livable_area +\n                      garage_spaces + \n                      Age + I(Age^2) + \n                      exterior_good,\n                     data = parcel_data,\n                       method = \"lm\",\n                       trControl = ctrl)\n\n  \nmodel_cv2 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     number_of_bedrooms + \n                     log_livable_area  +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     median_incomeE + \n                     percentage_bach + \n                     percentage_pov,\n                     data = parcel_data,\n                      method = \"lm\",\n                      trControl = ctrl\n)\n\n\nmodel_cv3 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms +\n                     garage_spaces +\n                     Age + I(Age^2) + \n                     exterior_good + \n                     log_livable_area +\n                     median_incomeE + \n                     percentage_bach +\n                     percentage_pov + \n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                    method = \"lm\",\n                    trControl = ctrl\n)\n\n\n\nmodel_cv4 &lt;- train(log_sale_price ~ \n                     number_of_bathrooms+\n                     garage_spaces + \n                     Age + I(Age^2) +\n                     exterior_good + \n                     log_livable_area * wealthy_neighborhood + \n                     median_incomeE +\n                     percentage_bach + \n                     percentage_pov +\n                     college_nn1 + \n                     violent_crime_600ft,\n                     data = parcel_data,\n                      method = \"lm\",\n                    trControl = ctrl\n)\n\n\nBased on each model’s measurement average squared prediction error (RMSE), average absolute difference between predicted and actual values (MAE), and reported proportion of explained variance (R^2), we determined that Model 4’s inclusion of structural, socioeconomic, spatial, and interactive effects all contributed to smaller errors between predicted values and actual values and offered greater explanatory power.\n\n\n\nTable 6. Cross-Validated Performance Metrics for Four Models\n\n\nModel\nRMSE\nRsquared\nMAE\n\n\n\n\nModel 1\n0.6077\n0.3508\n0.4540\n\n\nModel 2\n0.4952\n0.5690\n0.3540\n\n\nModel 3\n0.4834\n0.5894\n0.3417\n\n\nModel 4\n0.4769\n0.6001\n0.3354\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph of the actual logged sale prices by the predicted logged sale prices compliments our model performance metrics found in Table 6 as points cluster closely around the reference line suggesting stable consistency in errors and moderate ability to predict sale prices.\n\n\nPhase 6: Model Diagnostics\nWhen using linear regression to model relationships, we assume a number of conditions are met to support the validity of using a linear model. Here we test various linear modeling assumptions to determine the appropriateness of our model.\nOne linear model assumption is the assumption that the relationship is actually linear. If the linearity assumption is met, residuals should be randomly scattered around the zero line with no discernible pattern.\nIn Figure 7, we plotted the residuals or prediction errors by the model’s fitted values. The red dashed line at zero marks where residuals would fall if predictions were perfect. In this plot, the residuals appear to fan out as fitted values increase, suggesting a possible violation of the linearity. This pattern may indicate that the model under- or over-predicts at different price levels.\n\n\n\n\n\n\n\n\n\nAnother assumption we test is the normal distribution of residuals or errors. In Figure 8, we test the assumption of normally distributed residuals using a Q-Q plot. If normality of residual’s is met, the sample quantile points should fall roughly along the blue reference line representing theoretical quantiles of normal distribution.\nIn this plot, the residuals deviate from the line, particularly in the tails at low and high values, indicating non-normality.\n\n\n\n\n\n\n\n\n\nOne last regression assumption check we performed was determining whether there were influential outliers present. Figure 9 displays Cook’s Distance values for each observation, with colors indicating whether the point is flagged as influential, blue being none influential observations and red being the influential obeservations. A substantial large number of our observations were marked as influential, suggesting that many data points have both high leverage and large residuals.\n\n\n\n\n\n\n\n\n\n\n\nPhase 7: Conclusions & Recommendations\nThe final model demonstrates strong performance, achieving an adjusted R² of 0.591 and explaining nearly 59% of the variation in housing prices across Philadelphia. This marks a substantial improvement over the baseline structural model (Model 1, R² = 0.35), highlighting the value of incorporating spatial characteristics alongside structural and socioeconomic factors.\nAmong all predictors, total livable area emerges as the most influential driver of price, followed by the number of bathrooms and garage spaces. Exterior condition, median income, and educational attainment also show positive associations. Conversely, poverty rate and local crime density are negatively associated with price.\nThe interaction between living area and wealthy neighborhoods reveal that additional square footage may not have as much of an impact on price in high-value areas. Spatial dependencies are also evident in the model’s predictive accuracy as prediction errors show spatial patterns. The model performs best in mid-range price neighborhoods but struggles in areas like Nicetown, Fairhill, and Upper Kensington, where residual errors are higher. These disparities raise important equity concerns in the context of policy as these areas that are predisposed to under-prediction in modeling are also historical disadvantaged neighborhoods."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "data manipulation\nversion control platforms"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "data manipulation\nversion control platforms"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nessential dplyr functions and syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI still need help understanding the differences and uses of software we will be using"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe’re building foundations for understanding how to manipulate large tables of data"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\ncreating websites for storing information is a very interesting concept that I’ve never explored before"
  },
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#which-features-matter-most",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#which-features-matter-most",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Which Features Matter Most?",
    "text": "Which Features Matter Most?\n\n\n\n\n\n\n\n\nFeature\nDirection\nInterpretation\n\n\n\n\nLiving area\n↑\nStrongest driver of housing price\n\n\nAge + Age²\n↓ then ↑\nU-shaped pattern — older historic homes regain value\n\n\nExterior good\n↑\nMaintenance condition positively impacts price\n\n\nMedian income / Education\n↑\nSocioeconomic context drives demand\n\n\nPoverty rate / Crime\n↓\nNegative neighborhood effects\n\n\nInteraction: Living area × Wealthy neighborhood\n↓\nLarger homes add less"
  },
  {
    "objectID": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#thank-you-for-listening",
    "href": "assignments/assignment_3(midterm)/Rutherford_Angel_Presentation.html#thank-you-for-listening",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank you for listening",
    "text": "Thank you for listening\nAny questions?"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  }
]